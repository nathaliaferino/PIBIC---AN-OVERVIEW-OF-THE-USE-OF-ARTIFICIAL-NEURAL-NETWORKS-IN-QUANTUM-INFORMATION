{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYHxoVEIS6ExbXlqFdjnD9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nathaliaferino/PIBIC---AN-OVERVIEW-OF-THE-USE-OF-ARTIFICIAL-NEURAL-NETWORKS-IN-QUANTUM-INFORMATION/blob/main/RNA_TREINAMENTO_COM_ESTADOS_PUROS_SEPAR%C3%81VEIS_E_EMARANHADOS_COM_NEGATIVIDADE_NO_INTERVALO_(0_1%2C_0_2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rede Neural Artificial adaptada de:** UREÑA, Julio, et al. Entanglement detection with classical deep neural networks. Scientific Reports, 2024, 14.1: 18109."
      ],
      "metadata": {
        "id": "OxnkcIRuWidU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7b8_bmxVc04"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "from google.colab import files\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dados de para treino da RNA\n",
        "# Leitura dos dados\n",
        "treino_sep = pd.read_csv('/content/drive/MyDrive/GITHUB/Detecção de Emaranhamento Quântico/ESTADOS PUROS/TREINO/TREINO_PURO_SEPARAVEL.csv', header=None).to_numpy()\n",
        "treino_ema = pd.read_csv('/content/drive/MyDrive/GITHUB/Detecção de Emaranhamento Quântico/ESTADOS PUROS/TREINO_PURO_NEG_0.1_0.2.csv', header=None).to_numpy()\n",
        "\n",
        "# Estados separáveis: 0, Estados máximamente emaranhados: 1\n",
        "indice_zero = np.zeros((len(treino_sep), 1))\n",
        "indice_one = np.ones((len(treino_ema), 1))\n",
        "treino_sep = np.hstack((treino_sep, indice_zero))\n",
        "treino_ema = np.hstack((treino_ema, indice_one))\n",
        "\n",
        "# Embaralhando os dados\n",
        "treino = np.vstack((treino_sep, treino_ema))\n",
        "np.random.shuffle(treino)\n"
      ],
      "metadata": {
        "id": "vzFCzzPVWoAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#Estrutura da RNA\n",
        "class RNA_emaranhamento(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNA_emaranhamento, self).__init__()\n",
        "\n",
        "        # Camada de entrada\n",
        "        self.linear0 = nn.Linear(16, 800)\n",
        "\n",
        "        # Camadas ocultas\n",
        "        self.linear11 = nn.Linear(800,512)\n",
        "        self.linear1 = nn.Linear(512,256)\n",
        "        self.linear2 = nn.Linear(256,128)\n",
        "        self.linear3 = nn.Linear(128,16)\n",
        "\n",
        "        # Camada de saída\n",
        "        self.output = nn.Linear(16, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #Camada de entrada\n",
        "        x = self.linear0(x)\n",
        "\n",
        "        # Camadas ocultas com ReLU\n",
        "        x = F.relu(self.linear11(x))\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.relu(self.linear3(x))\n",
        "\n",
        "        # Camada de saída com sigmoide\n",
        "        y_predicted = torch.sigmoid(self.output(x))\n",
        "        return y_predicted\n",
        "\n",
        "#Otimizador e função custo\n",
        "model = RNA_emaranhamento().to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.NAdam(\n",
        "    model.parameters(),\n",
        "    lr=0.001,\n",
        "    betas=(0.9, 0.999),\n",
        "    eps=1e-8,\n",
        "    weight_decay=0,\n",
        "    momentum_decay=4e-3\n",
        ")"
      ],
      "metadata": {
        "id": "HYTpZLvVW3Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = treino[:, :16]\n",
        "y = treino[:, 16].reshape(-1, 1)\n",
        "\n",
        "# Normalização dos valores de entrada\n",
        "mean = np.mean(x, axis=0, keepdims=True)\n",
        "std  = np.std(x, axis=0, keepdims=True)\n",
        "std = np.where(std < 1e-12, 1e-12, std)\n",
        "x = (x - mean) / std\n",
        "\n",
        "# Conversão para tensor\n",
        "x = torch.from_numpy(x).float().to(device)\n",
        "y = torch.from_numpy(y).float().to(device)\n",
        "\n",
        "# Separando os dados em lotes\n",
        "tamanho_do_lote = 100\n",
        "dataset = TensorDataset(x, y)\n",
        "loader = DataLoader(dataset, batch_size=tamanho_do_lote, shuffle=True)\n",
        "num_lotes = x.shape[0] // tamanho_do_lote\n",
        "\n",
        "# Quantidade de iterações\n",
        "num_epocas = 1000\n",
        "\n",
        "# Perda\n",
        "losses = []\n",
        "\n",
        "# Treinamento\n",
        "model.train()\n",
        "for epoca in range(num_epocas):\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for x_lote, y_lote in loader:\n",
        "        # Forward\n",
        "        y_pred = model(x_lote)\n",
        "\n",
        "        # Loss\n",
        "        loss = criterion(y_pred, y_lote)\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # média da época\n",
        "    epoch_loss /= len(loader)\n",
        "    losses.append(epoch_loss)\n",
        "\n",
        "    if (epoca + 1) % 100 == 0:\n",
        "        print(f\"Época [{epoca+1}/{num_epocas}], Loss: {epoch_loss:.6f}\")\n",
        "\n",
        "print(\"\\nTreinamento concluído.\")"
      ],
      "metadata": {
        "id": "ket8Uy3YW6uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Velocidade de convergência da RNA\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Época', fontsize=14)\n",
        "plt.ylabel('Binary Cross-Entropy)', fontsize=14)\n",
        "plt.grid(True)\n",
        "plt.ylim(0.0, 1.0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GFW9z--QW_K-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ler dados para teste da RNA\n",
        "Pteste_sep = pd.read_csv('/content/drive/MyDrive/GITHUB/Detecção de Emaranhamento Quântico/ESTADOS PUROS/TESTE/TESTE_PURO_SEPARAVEL.csv', header=None).to_numpy()\n",
        "Pteste_emar_0_1 = pd.read_csv('/content/drive/MyDrive/GITHUB/Detecção de Emaranhamento Quântico/ESTADOS PUROS/TESTE/TESTE_PURO_NEG_0.0_0.1.csv', header=None).to_numpy()\n",
        "Pteste_emar_1_2 = pd.read_csv('/content/drive/MyDrive/GITHUB/Detecção de Emaranhamento Quântico/ESTADOS PUROS/TESTE/TESTE_PURO_NEG_0.1_0.2.csv', header=None).to_numpy()\n",
        "Pteste_emar_2_3 = pd.read_csv('/content/drive/MyDrive/GITHUB/Detecção de Emaranhamento Quântico/ESTADOS PUROS/TESTE/TESTE_PURO_NEG_0.2_0.3.csv', header=None).to_numpy()\n",
        "Pteste_emar_3_4 = pd.read_csv('/content/drive/MyDrive/GITHUB/Detecção de Emaranhamento Quântico/ESTADOS PUROS/TESTE/TESTE_PURO_NEG_0.3_0.4.csv', header=None).to_numpy()\n",
        "Pteste_emar_4_5 = pd.read_csv('/content/drive/MyDrive/GITHUB/Detecção de Emaranhamento Quântico/ESTADOS PUROS/TESTE/TESTE_PURO_NEG_0.4_0.5.csv', header=None).to_numpy()\n",
        "\n",
        "Mteste_sep = pd.read_csv('/content/drive/MyDrive/GITHUB/Detecção de Emaranhamento Quântico/ESTADOS MISTOS/TESTE/TESTE_MISTO_SEPARAVEL.csv', header=None).to_numpy()\n",
        "Mteste_emar_0_1 = pd.read_csv('/content/drive/MyDrive/GITHUB/Detecção de Emaranhamento Quântico/ESTADOS MISTOS/TESTE/TESTE_MISTO_NEG_0.0_0.1.csv', header=None).to_numpy()\n",
        "Mteste_emar_1_2 = pd.read_csv('/content/drive/MyDrive/GITHUB/Detecção de Emaranhamento Quântico/ESTADOS MISTOS/TESTE/TESTE_MISTO_NEG_0.1_0.2.csv', header=None).to_numpy()\n",
        "Mteste_emar_2_3 = pd.read_csv('/content/drive/MyDrive/GITHUB/Detecção de Emaranhamento Quântico/ESTADOS MISTOS/TESTE/TESTE_MISTO_NEG_0.2_0.3.csv', header=None).to_numpy()\n",
        "Mteste_emar_3_4 = pd.read_csv('/content/drive/MyDrive/GITHUB/Detecção de Emaranhamento Quântico/ESTADOS MISTOS/TESTE/TESTE_MISTO_NEG_0.3_0.4.csv', header=None).to_numpy()\n",
        "Mteste_emar_4_5 = pd.read_csv('/content/drive/MyDrive/GITHUB/Detecção de Emaranhamento Quântico/ESTADOS MISTOS/TESTE/TESTE_MISTO_NEG_0.4_0.5.csv', header=None).to_numpy()\n",
        "\n",
        "\n",
        "def separar_conjuntos_de_testes(separavel,emaranhado):\n",
        "  indice_zero = np.zeros((len(separavel), 1))\n",
        "  indice_one = np.ones((len(emaranhado), 1))\n",
        "\n",
        "  separavel = np.hstack((separavel, indice_zero))\n",
        "  emaranhado = np.hstack((emaranhado, indice_one))\n",
        "\n",
        "  dados_teste = np.vstack((separavel, emaranhado))\n",
        "  np.random.shuffle(dados_teste)\n",
        "  return dados_teste\n",
        "\n",
        "Pnegatividade0_1 = separar_conjuntos_de_testes(Pteste_sep, Pteste_emar_0_1)\n",
        "Pnegatividade1_2 = separar_conjuntos_de_testes(Pteste_sep, Pteste_emar_1_2)\n",
        "Pnegatividade2_3 = separar_conjuntos_de_testes(Pteste_sep, Pteste_emar_2_3)\n",
        "Pnegatividade3_4 = separar_conjuntos_de_testes(Pteste_sep, Pteste_emar_3_4)\n",
        "Pnegatividade4_5 = separar_conjuntos_de_testes(Pteste_sep, Pteste_emar_4_5)\n",
        "\n",
        "Mnegatividade0_1 = separar_conjuntos_de_testes(Mteste_sep, Mteste_emar_0_1)\n",
        "Mnegatividade1_2 = separar_conjuntos_de_testes(Mteste_sep, Mteste_emar_1_2)\n",
        "Mnegatividade2_3 = separar_conjuntos_de_testes(Mteste_sep, Mteste_emar_2_3)\n",
        "Mnegatividade3_4 = separar_conjuntos_de_testes(Mteste_sep, Mteste_emar_3_4)\n",
        "Mnegatividade4_5 = separar_conjuntos_de_testes(Mteste_sep, Mteste_emar_4_5)\n"
      ],
      "metadata": {
        "id": "40cN-08CXAFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calcular_taxa_acerto(dados, mean1, std1):\n",
        "    # Separação entrada/saída\n",
        "    x_teste = dados[:, :16]\n",
        "    y_teste = dados[:, 16].reshape(-1, 1)\n",
        "\n",
        "    # Normalização Z-score\n",
        "    std_safe1 = np.where(std1 < 1e-12, 1e-12, std1)\n",
        "    x_teste = (x_teste - mean1) / std_safe1\n",
        "\n",
        "    # Converte para tensor\n",
        "    x_teste = torch.from_numpy(x_teste).float().to(device)\n",
        "    y_teste = torch.from_numpy(y_teste).float().to(device)\n",
        "\n",
        "    # Avaliação do modelo\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred_proba = model(x_teste)\n",
        "\n",
        "        # Classe 1 se >= 0.5\n",
        "        y_pred_classes = (y_pred_proba >= 0.5).float()\n",
        "\n",
        "        acertos = (y_pred_classes == y_teste).sum().item()\n",
        "        total_amostras = y_teste.shape[0]\n",
        "        taxa_acerto = (acertos / total_amostras) * 100\n",
        "\n",
        "        print(f\"Total de amostras de teste: {total_amostras}\")\n",
        "        print(f\"Número de acertos: {acertos}\")\n",
        "        print(f\"Taxa de Acerto: {taxa_acerto:.2f}%\")"
      ],
      "metadata": {
        "id": "bgby1YZTXCRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ESTADOS PUROS')\n",
        "print('NEGATIVIDADE (0.0,0.1)')\n",
        "calcular_taxa_acerto(Pnegatividade0_1, mean, std)\n",
        "print('NEGATIVIDADE (0.1,0.2)')\n",
        "calcular_taxa_acerto(Pnegatividade1_2, mean, std)\n",
        "print('NEGATIVIDADE (0.2,0.3)')\n",
        "calcular_taxa_acerto(Pnegatividade2_3, mean, std)\n",
        "print('NEGATIVIDADE (0.3,0.4)')\n",
        "calcular_taxa_acerto(Pnegatividade3_4, mean, std)\n",
        "print('NEGATIVIDADE (0.4,0.5)')\n",
        "calcular_taxa_acerto(Pnegatividade4_5, mean, std)"
      ],
      "metadata": {
        "id": "xVHyukAdXEba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ESTADOS MISTOS')\n",
        "print('NEGATIVIDADE (0.0,0.1)')\n",
        "calcular_taxa_acerto(Mnegatividade0_1, mean, std)\n",
        "print('NEGATIVIDADE (0.1,0.2)')\n",
        "calcular_taxa_acerto(Mnegatividade1_2, mean, std)\n",
        "print('NEGATIVIDADE (0.2,0.3)')\n",
        "calcular_taxa_acerto(Mnegatividade2_3, mean, std)\n",
        "print('NEGATIVIDADE (0.3,0.4)')\n",
        "calcular_taxa_acerto(Mnegatividade3_4, mean, std)\n",
        "print('NEGATIVIDADE (0.4,0.5)')\n",
        "calcular_taxa_acerto(Mnegatividade4_5, mean, std)"
      ],
      "metadata": {
        "id": "EihZi4h1XGkW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}